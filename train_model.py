import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import json
from sklearn.model_selection import train_test_split

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
IMG_SIZE = 48
BATCH_SIZE = 64
EPOCHS = 20
NUM_CLASSES = 7

EMOTIONS = {
    0: "Angry",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Sad",
    5: "Surprise",
    6: "Neutral"
}


def load_fer2013_dataset(csv_path='fer2013/fer2013.csv'):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ FER2013"""
    df = pd.read_csv(csv_path)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–∏–∫—Å–µ–ª–∏ –∏ –º–µ—Ç–∫–∏
    pixels = df['pixels'].tolist()
    emotions = df['emotion'].tolist()
    usage = df['Usage'].tolist()

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–∏–∫—Å–µ–ª–µ–π –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    images = []
    for pixel_sequence in pixels:
        pixels_array = np.array(pixel_sequence.split(' '), dtype=np.float32)
        image = pixels_array.reshape(IMG_SIZE, IMG_SIZE)
        images.append(image)

    images = np.array(images) / 255.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    images = np.expand_dims(images, -1)  # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª
    emotions = np.array(emotions)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test
    train_indices = [i for i, u in enumerate(usage) if u == 'Training']
    val_indices = [i for i, u in enumerate(usage) if u == 'PublicTest']
    test_indices = [i for i, u in enumerate(usage) if u == 'PrivateTest']

    X_train = images[train_indices]
    y_train = emotions[train_indices]
    X_val = images[val_indices]
    y_val = emotions[val_indices]
    X_test = images[test_indices]
    y_test = emotions[test_indices]

    return (X_train, y_train), (X_val, y_val), (X_test, y_test)


def create_emotion_model():
    """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π"""

    model = models.Sequential([
        # –ü–µ—Ä–≤—ã–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv2D(64, (3, 3), activation='relu', padding='same',
                      input_shape=(IMG_SIZE, IMG_SIZE, 1)),
        layers.BatchNormalization(),
        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # –í—Ç–æ—Ä–æ–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # –¢—Ä–µ—Ç–∏–π —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–π –±–ª–æ–∫
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
        layers.BatchNormalization(),
        layers.MaxPooling2D((2, 2)),
        layers.Dropout(0.25),

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        layers.Flatten(),
        layers.Dense(512, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.5),

        layers.Dense(256, activation='relu',
                     kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Dropout(0.5),

        layers.Dense(NUM_CLASSES, activation='softmax')
    ])

    return model


def create_data_augmentation():
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    return ImageDataGenerator(
        rotation_range=10,
        zoom_range=0.1,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        horizontal_flip=True,
        fill_mode='nearest'
    )


def train():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"""

    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ FER2013...")
    (X_train, y_train), (X_val, y_val), (X_test, y_test) = load_fer2013_dataset()

    print(f"üìà –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"   –û–±—É—á–∞—é—â–∏–µ: {X_train.shape}")
    print(f"   –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ: {X_val.shape}")
    print(f"   –¢–µ—Å—Ç–æ–≤—ã–µ: {X_test.shape}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model = create_emotion_model()

    # –ö–æ–º–ø–∏–ª—è—Ü–∏—è
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    # –ö–æ–ª–ª–±—ç–∫–∏
    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001),
        keras.callbacks.ModelCheckpoint(
            'models/best_emotion_model.h5',
            monitor='val_accuracy',
            save_best_only=True,
            mode='max'
        )
    ]

    # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    datagen = create_data_augmentation()
    train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE)

    # –û–±—É—á–µ–Ω–∏–µ
    print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=(X_val, y_val),
        callbacks=callbacks,
        verbose=1
    )

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model.save('models/emotion_model.h5')
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'models/emotion_model.h5'")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤
    with open('models/emotion_labels.json', 'w') as f:
        json.dump(EMOTIONS, f)
    print("üíæ –ú–µ—Ç–∫–∏ –∫–ª–∞—Å—Å–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'models/emotion_labels.json'")

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
    print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {test_acc:.4f}")
    print(f"   –ü–æ—Ç–µ—Ä–∏: {test_loss:.4f}")

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    plot_training_history(history)

    return model, history


def plot_training_history(history):
    """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))

    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
    axes[0].plot(history.history['accuracy'], label='Train Accuracy')
    axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')
    axes[0].set_title('–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
    axes[0].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
    axes[0].legend()
    axes[0].grid(True)

    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    axes[1].plot(history.history['loss'], label='Train Loss')
    axes[1].plot(history.history['val_loss'], label='Val Loss')
    axes[1].set_title('–ü–æ—Ç–µ—Ä–∏ –º–æ–¥–µ–ª–∏')
    axes[1].set_xlabel('–≠–ø–æ—Ö–∞')
    axes[1].set_ylabel('–ü–æ—Ç–µ—Ä–∏')
    axes[1].legend()
    axes[1].grid(True)

    plt.tight_layout()
    plt.savefig('models/training_history.png', dpi=150)
    plt.show()


if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    os.makedirs('models', exist_ok=True)
    os.makedirs('fer2013', exist_ok=True)

    print("üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π")
    print("=" * 50)

    try:
        model, history = train()
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        model.summary()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        print("\nüí° –ó–∞–º–µ—á–∞–Ω–∏–µ: –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–∫–∞—á–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç FER2013")
        print("   –∏ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç–µ —Ñ–∞–π–ª fer2013.csv –≤ –ø–∞–ø–∫–µ fer2013/")