import tensorflow as tf
import keras
import cv2
import numpy as np
import os
import json
import uuid
import math
from datetime import datetime
from fastapi import FastAPI, File, UploadFile, Request, Form
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from typing import List, Dict, Optional
from pydantic import BaseModel

app = FastAPI(title="–°–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π")

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# –≠–º–æ—Ü–∏–∏ –∏ —Ü–≤–µ—Ç–∞
EMOTION_CLASSES = [
    'Angry',
    'Disgust',
    'Fear',
    'Happy',
    'Sad',
    'Surprise',
    'Neutral'
]

# –¶–≤–µ—Ç–∞ –≤ BGR –¥–ª—è OpenCV
EMOTION_COLORS_BGR = {
    'Angry': (0, 0, 255),
    'Disgust': (0, 128, 0),
    'Fear': (128, 0, 128),
    'Happy': (139, 219, 255),
    'Sad': (255, 0, 0),
    'Surprise': (65, 184, 255),
    'Neutral': (128, 128, 128),
    'default': (0, 255, 0)
}

EMOTION_COLORS_RGB = {
    'Angry': (255, 0, 0),
    'Disgust': (0, 128, 0),
    'Fear': (128, 0, 128),
    'Happy': (255, 219, 139),
    'Sad': (0, 0, 255),
    'Surprise': (255, 184, 65),
    'Neutral': (128, 128, 128),
    'default': (0, 255, 0)
}

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤ —à–∞–±–ª–æ–Ω–∞—Ö
EMOTION_COLORS = EMOTION_COLORS_RGB

MODEL_PATH = "models/emotion_model.h5"
LABELS_PATH = "models/emotion_labels.json"

# === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===
try:
    emotion_model = keras.saving.load_model(MODEL_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ keras.saving.load_model")

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    emotion_model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    MODEL_LOADED = True
    print("–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: 62.9%")

except Exception as e1:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Keras 3: {e1}")

    try:
        # –°–ø–æ—Å–æ–± 2: –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ tf.keras (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        emotion_model = tf.keras.models.load_model(MODEL_PATH, compile=False)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ tf.keras")
        MODEL_LOADED = True
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ tf.keras: {e2}")

        # –°–ø–æ—Å–æ–± 3: –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
        print("üí° –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–º–æ–¥–µ–ª—å...")
        emotion_model = keras.Sequential([
            keras.layers.Input(shape=(48, 48, 1)),
            keras.layers.Conv2D(32, (3, 3), activation='relu'),
            keras.layers.GlobalAveragePooling2D(),
            keras.layers.Dense(7, activation='softmax')
        ])

        emotion_model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        MODEL_LOADED = True
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–µ–º–æ-–º–æ–¥–µ–ª—å")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∫–∏ —ç–º–æ—Ü–∏–π
try:
    if os.path.exists(LABELS_PATH):
        with open(LABELS_PATH, 'r') as f:
            emotion_labels = json.load(f)
    else:
        emotion_labels = {
            "0": "Angry", "1": "Disgust", "2": "Fear",
            "3": "Happy", "4": "Sad", "5": "Surprise", "6": "Neutral"
        }
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∫–∏: {len(emotion_labels)} —ç–º–æ—Ü–∏–π")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–æ–∫: {e}")
    emotion_labels = {
        "0": "Angry", "1": "Disgust", "2": "Fear",
        "3": "Happy", "4": "Sad", "5": "Surprise", "6": "Neutral"
    }


# ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

def convert_to_serializable(obj):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã NumPy –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã"""
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_to_serializable(item) for item in obj)
    elif hasattr(obj, '__dict__'):
        return convert_to_serializable(obj.__dict__)
    else:
        return obj


def safe_json_dumps(obj):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ JSON"""
    return json.dumps(convert_to_serializable(obj), ensure_ascii=False)


def get_color_for_emotion(emotion_name, format='bgr'):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –¥–ª—è —ç–º–æ—Ü–∏–∏"""
    if format == 'bgr':
        return EMOTION_COLORS_BGR.get(emotion_name, EMOTION_COLORS_BGR['default'])
    else:
        return EMOTION_COLORS_RGB.get(emotion_name, EMOTION_COLORS_RGB['default'])


def detect_faces(image):
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü"""
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    return faces


def preprocess_face_for_model(face_image):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ª–∏—Ü–∞ –¥–ª—è –º–æ–¥–µ–ª–∏"""
    try:
        if len(face_image.shape) == 3:
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = face_image

        # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        gray = cv2.equalizeHist(gray)
        gray = cv2.GaussianBlur(gray, (3, 3), 0)

        # –†–µ—Å–∞–π–∑ –¥–æ 48x48
        resized = cv2.resize(gray, (48, 48))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        normalized = resized.astype('float32') / 255.0

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        return np.expand_dims(normalized, axis=(0, -1))

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Ü–∞: {e}")
        return np.ones((1, 48, 48, 1)) * 0.5


def predict_emotion_model(face_image):
    """–ù–∞—Å—Ç–æ—è—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Keras –º–æ–¥–µ–ª–∏"""
    if face_image.size == 0:
        return {
            "emotion": "Unknown",
            "confidence": 0.0,
            "all_predictions": []
        }

    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        processed = preprocess_face_for_model(face_image)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = emotion_model.predict(processed, verbose=0)

        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é —ç–º–æ—Ü–∏—é
        emotion_idx = np.argmax(predictions[0])
        confidence = float(predictions[0][emotion_idx])

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏
        emotion_name = emotion_labels.get(str(emotion_idx), f"Emotion_{emotion_idx}")

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_predictions = []
        for i, prob in enumerate(predictions[0]):
            emo_name = emotion_labels.get(str(i), f"Emotion_{i}")
            all_predictions.append({
                "emotion": emo_name,
                "probability": float(prob),
                "color_bgr": get_color_for_emotion(emo_name, 'bgr'),
                "color_rgb": get_color_for_emotion(emo_name, 'rgb')
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º
        all_predictions.sort(key=lambda x: x["probability"], reverse=True)

        print(f"üé≠ –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞: {emotion_name} ({confidence:.2%})")

        return {
            "emotion": emotion_name,
            "confidence": confidence,
            "emotion_idx": int(emotion_idx),
            "color_bgr": get_color_for_emotion(emotion_name, 'bgr'),
            "color_rgb": get_color_for_emotion(emotion_name, 'rgb'),
            "all_predictions": all_predictions
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        # –í–º–µ—Å—Ç–æ –¥–µ–º–æ-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤–æ–µ
        return {
            "emotion": "Neutral",
            "confidence": 0.5,
            "color_bgr": get_color_for_emotion("Neutral", 'bgr'),
            "color_rgb": get_color_for_emotion("Neutral", 'rgb'),
            "all_predictions": [{"emotion": "Neutral", "probability": 1.0}]
        }


def predict_emotion(face_image):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    if MODEL_LOADED and emotion_model is not None:
        return predict_emotion_model(face_image)
    else:
        return {
            "emotion": "Neutral",
            "confidence": 0.5,
            "color_bgr": get_color_for_emotion("Neutral", 'bgr'),
            "color_rgb": get_color_for_emotion("Neutral", 'rgb'),
            "all_predictions": [{"emotion": "Neutral", "probability": 1.0}]
        }


# ==================== API ENDPOINTS ====================

@app.get("/")
async def home(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return templates.TemplateResponse("emotion_detection.html", {
        "request": request,
        "model_loaded": MODEL_LOADED,
        "emotions": emotion_labels,
        "emotion_colors": EMOTION_COLORS_RGB,
        "default_threshold": 50
    })


@app.post("/detect")
async def detect_emotions(
        request: Request,
        file: UploadFile = File(...),
        confidence_threshold: float = Form(50.0),
        selected_emotions: str = Form(""),
        calculate_area: bool = Form(True)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –í–ö–õ–Æ–ß–ï–ù–û
):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π"""

    if not MODEL_LOADED:
        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "error": "–ú–æ–¥–µ–ª—å —ç–º–æ—Ü–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!"
        })

    try:
        threshold = confidence_threshold / 100.0

        if selected_emotions:
            selected_emotions_list = [emo.strip().lower() for emo in selected_emotions.split(",")]
            valid_emotions = [emo for emo in selected_emotions_list
                              if emo in [e.lower() for e in EMOTION_CLASSES]]
        else:
            valid_emotions = []

        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        contents = await file.read()
        nparr = np.frombuffer(contents, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        original_image = image.copy()
        height, width = image.shape[:2]

        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –ª–∏—Ü–∞
        faces = detect_faces(image)
        detected_faces = []
        face_areas = []

        for i, (x, y, w, h) in enumerate(faces):
            if w < 20 or h < 20:
                continue

            face_roi = image[y:y + h, x:x + w]
            emotion_result = predict_emotion(face_roi)

            if emotion_result["confidence"] >= threshold:
                if valid_emotions and emotion_result["emotion"].lower() not in valid_emotions:
                    continue

                # –í–°–ï–ì–î–ê —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–ª–æ—â–∞–¥—å (–±–µ–∑ —É—Å–ª–æ–≤–∏—è if calculate_area)
                area_pixels = float(w * h)
                relative_area_percent = float((area_pixels / (width * height)) * 100)

                face_data = {
                    "face_id": i + 1,
                    "emotion": emotion_result["emotion"],
                    "confidence": float(emotion_result["confidence"]),
                    "box": [int(x), int(y), int(x + w), int(y + h)],
                    "color_bgr": emotion_result["color_bgr"],
                    "color_rgb": emotion_result["color_rgb"],
                    "all_predictions": emotion_result["all_predictions"],
                    # –í–°–ï–ì–î–ê –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    "area_pixels": area_pixels,
                    "relative_area_percent": relative_area_percent,
                    "aspect_ratio": float(w / h) if h > 0 else 0.0,
                    "width": w,
                    "height": h,
                    "center_x": x + w // 2,
                    "center_y": y + h // 2
                }

                face_areas.append(area_pixels)
                detected_faces.append(face_data)

        # –†–∏—Å—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for face in detected_faces:
            x_min, y_min, x_max, y_max = face["box"]
            color_bgr = face["color_bgr"]

            # –ü—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫ –≤–æ–∫—Ä—É–≥ –ª–∏—Ü–∞
            cv2.rectangle(original_image, (x_min, y_min), (x_max, y_max), color_bgr, 3)

            # –ü–æ–¥–ø–∏—Å—å —Å —ç–º–æ—Ü–∏–µ–π
            label = f"{face['face_id']}: {face['emotion']}: {face['confidence']:.2f}"
            (text_width, text_height), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
            )

            cv2.rectangle(original_image,
                          (x_min, y_min - text_height - 10),
                          (x_min + text_width, y_min),
                          color_bgr, -1)

            cv2.putText(original_image, label, (x_min, y_min - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        uploads_dir = "static/uploads"
        os.makedirs(uploads_dir, exist_ok=True)

        unique_id = str(uuid.uuid4())[:8]
        output_filename = f"detected_{unique_id}.jpg"
        original_filename = f"original_{unique_id}.jpg"

        cv2.imwrite(f"{uploads_dir}/{output_filename}", original_image)
        cv2.imwrite(f"{uploads_dir}/{original_filename}", image)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        emotion_stats = {}
        for face in detected_faces:
            emotion = face["emotion"]
            emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1

        image_info = {
            "width": int(width),
            "height": int(height),
            "format": file.content_type,
            "filename": file.filename
        }

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–ª–æ—â–∞–¥—è–º (–í–°–ï–ì–î–ê —Å—á–∏—Ç–∞–µ–º)
        area_stats = {
            "total_area_pixels": float(sum(face_areas)) if face_areas else 0,
            "average_area_pixels": float(sum(face_areas) / len(face_areas)) if face_areas else 0,
            "min_area_pixels": float(min(face_areas)) if face_areas else 0,
            "max_area_pixels": float(max(face_areas)) if face_areas else 0,
            "image_area_pixels": float(width * height),
            "faces_coverage_percent": float((sum(face_areas) / (width * height)) * 100) if face_areas and (
                        width * height) > 0 else 0
        }

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            "total_faces_detected": len(faces),
            "faces_with_emotion": len(detected_faces),
            "min_confidence": f"{min([f['confidence'] for f in detected_faces]) * 100:.1f}%" if detected_faces else "0%",
            "image_size": f"{width}x{height}",
            "processing_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "detected_faces": convert_to_serializable(detected_faces),
            "emotion_stats": convert_to_serializable(emotion_stats),
            "image_url": f"/static/uploads/{output_filename}",
            "original_image_url": f"/static/uploads/{original_filename}",
            "total_detected": len(detected_faces),
            "total_faces": len(faces),
            "image_info": convert_to_serializable(image_info),
            "used_threshold": confidence_threshold,
            "used_emotions": ", ".join(valid_emotions) if valid_emotions else "–≤—Å–µ —ç–º–æ—Ü–∏–∏",
            "calculate_area": True,  # –í—Å–µ–≥–¥–∞ True —Ç–µ–ø–µ—Ä—å
            "area_stats": convert_to_serializable(area_stats),
            "processing_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion_colors": EMOTION_COLORS_RGB,
            "stats": convert_to_serializable(stats),
            "results": convert_to_serializable(detected_faces),
            "emotion_distribution": convert_to_serializable(emotion_stats)
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
        })


# ==================== API –î–õ–Ø –ü–†–û–ì–†–ê–ú–ú–ò–°–¢–û–í ====================

class FaceBox(BaseModel):
    x: int
    y: int
    width: int
    height: int


class CalculateFaceRequest(BaseModel):
    image_width: int
    image_height: int
    face_box: FaceBox
    unit: str = "pixels"


class MultipleFacesRequest(BaseModel):
    image_width: int
    image_height: int
    faces: List[FaceBox]
    unit: str = "pixels"


@app.post("/api/calculate-face-area")
async def api_calculate_face_area(request: CalculateFaceRequest):
    """API –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–ª–æ—â–∞–¥–∏ –æ–¥–Ω–æ–≥–æ –ª–∏—Ü–∞"""
    area_pixels = request.face_box.width * request.face_box.height
    image_area = request.image_width * request.image_height

    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –µ–¥–∏–Ω–∏—Ü
    if request.unit == "cm":
        area = area_pixels * 0.0007  # –ø—Ä–∏–º–µ—Ä–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
        unit = "cm¬≤"
    elif request.unit == "inches":
        area = area_pixels * 0.0001089
        unit = "in¬≤"
    else:
        area = area_pixels
        unit = "px¬≤"

    return {
        "area": {
            "pixels": area_pixels,
            "converted": area,
            "unit": unit,
            "relative_percent": round((area_pixels / image_area * 100), 2) if image_area > 0 else 0
        },
        "center": {
            "x": request.face_box.x + request.face_box.width // 2,
            "y": request.face_box.y + request.face_box.height // 2
        },
        "aspect_ratio": request.face_box.width / request.face_box.height if request.face_box.height > 0 else 0,
        "diagonal": math.sqrt(request.face_box.width ** 2 + request.face_box.height ** 2)
    }


@app.post("/api/calculate-multiple-faces-area")
async def api_calculate_multiple_faces_area(request: MultipleFacesRequest):
    """API –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø–ª–æ—â–∞–¥–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–∏—Ü"""
    results = []
    total_area_pixels = 0

    for face in request.faces:
        area_pixels = face.width * face.height
        total_area_pixels += area_pixels

        if request.unit == "cm":
            area = area_pixels * 0.0007
            unit = "cm¬≤"
        elif request.unit == "inches":
            area = area_pixels * 0.0001089
            unit = "in¬≤"
        else:
            area = area_pixels
            unit = "px¬≤"

        results.append({
            "box": face.dict(),
            "area_in_pixels": area_pixels,
            "area_in_requested_unit": area,
            "unit": unit,
            "center_x": face.x + face.width // 2,
            "center_y": face.y + face.height // 2
        })

    image_area = request.image_width * request.image_height

    return {
        "faces": results,
        "summary": {
            "total_faces": len(request.faces),
            "total_area_pixels": total_area_pixels,
            "coverage_percentage": round((total_area_pixels / image_area * 100), 2) if image_area > 0 else 0,
            "average_area_pixels": total_area_pixels / len(request.faces) if request.faces else 0
        }
    }


@app.get("/api/stats")
async def api_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "system": {
            "status": "running",
            "model_loaded": MODEL_LOADED,
            "model_accuracy": 0.629,
            "total_emotions": len(emotion_labels)
        },
        "endpoints": [
            "/api/calculate-face-area",
            "/api/calculate-multiple-faces-area",
            "/api/stats",
            "/detect"
        ]
    }


@app.get("/api/detect-and-calculate/{image_id}")
async def detect_and_calculate(image_id: str):
    """API –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ª–∏—Ü –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏"""
    # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –ª–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ ID
    return {
        "image_id": image_id,
        "message": "–î–ª—è —Ä–∞–±–æ—Ç—ã —ç—Ç–æ–≥–æ endpoint —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π",
        "available_endpoints": [
            "/api/calculate-face-area",
            "/api/calculate-multiple-faces-area",
            "/detect (POST)"
        ]
    }


if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("üöÄ –°–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –∑–∞–ø—É—â–µ–Ω–∞!")
    print(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {'‚úÖ' if MODEL_LOADED else '‚ùå'}")
    if MODEL_LOADED:
        print(f"üé≠ –≠–º–æ—Ü–∏–π: {len(emotion_labels)}")
        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: ~63%")
    print("üåê –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å: http://localhost:8000")
    print("üîß Swagger UI: http://localhost:8000/docs")
    print("üìö ReDoc: http://localhost:8000/redoc")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8000)
