import tensorflow as tf
import keras
import cv2
import numpy as np
import os
import json
import uuid
from datetime import datetime
from fastapi import FastAPI, File, UploadFile, Request, Form
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from typing import List, Dict, Optional
from pydantic import BaseModel

app = FastAPI(title="–°–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π")

templates = Jinja2Templates(directory="templates")
app.mount("/static", StaticFiles(directory="static"), name="static")

# –≠–º–æ—Ü–∏–∏ –∏ —Ü–≤–µ—Ç–∞
EMOTION_CLASSES = [
    'Angry',
    'Disgust',
    'Fear',
    'Happy',
    'Sad',
    'Surprise',
    'Neutral'
]

# –¶–≤–µ—Ç–∞ –≤ BGR –¥–ª—è OpenCV
EMOTION_COLORS_BGR = {
    'Angry': (0, 0, 255),
    'Disgust': (0, 128, 0),
    'Fear': (128, 0, 128),
    'Happy': (139, 219, 255),
    'Sad': (255, 0, 0),
    'Surprise': (65, 184, 255),
    'Neutral': (128, 128, 128),
    'default': (0, 255, 0)
}

EMOTION_COLORS_RGB = {
    'Angry': (255, 0, 0),
    'Disgust': (0, 128, 0),
    'Fear': (128, 0, 128),
    'Happy': (255, 219, 139),
    'Sad': (0, 0, 255),
    'Surprise': (255, 184, 65),
    'Neutral': (128, 128, 128),
    'default': (0, 255, 0)
}

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤ —à–∞–±–ª–æ–Ω–∞—Ö
EMOTION_COLORS = EMOTION_COLORS_RGB

MODEL_PATH = "models/emotion_model.h5"
LABELS_PATH = "models/emotion_labels.json"

# === –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò ===
try:
    emotion_model = keras.saving.load_model(MODEL_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ keras.saving.load_model")

    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    emotion_model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    MODEL_LOADED = True
    print("–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: 62.9%")

except Exception as e1:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Keras 3: {e1}")

    try:
        # –°–ø–æ—Å–æ–± 2: –ü–æ–ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ tf.keras (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        emotion_model = tf.keras.models.load_model(MODEL_PATH, compile=False)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ tf.keras")
        MODEL_LOADED = True
    except Exception as e2:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ tf.keras: {e2}")

        # –°–ø–æ—Å–æ–± 3: –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å
        print("üí° –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–º–æ–¥–µ–ª—å...")
        emotion_model = keras.Sequential([
            keras.layers.Input(shape=(48, 48, 1)),
            keras.layers.Conv2D(32, (3, 3), activation='relu'),
            keras.layers.GlobalAveragePooling2D(),
            keras.layers.Dense(7, activation='softmax')
        ])

        emotion_model.compile(
            optimizer='adam',
            loss='sparse_categorical_crossentropy',
            metrics=['accuracy']
        )

        MODEL_LOADED = True
        print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –¥–µ–º–æ-–º–æ–¥–µ–ª—å")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∫–∏ —ç–º–æ—Ü–∏–π
try:
    if os.path.exists(LABELS_PATH):
        with open(LABELS_PATH, 'r') as f:
            emotion_labels = json.load(f)
    else:
        emotion_labels = {
            "0": "Angry", "1": "Disgust", "2": "Fear",
            "3": "Happy", "4": "Sad", "5": "Surprise", "6": "Neutral"
        }
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –º–µ—Ç–∫–∏: {len(emotion_labels)} —ç–º–æ—Ü–∏–π")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–æ–∫: {e}")
    emotion_labels = {
        "0": "Angry", "1": "Disgust", "2": "Fear",
        "3": "Happy", "4": "Sad", "5": "Surprise", "6": "Neutral"
    }


# ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ====================

def convert_to_serializable(obj):
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç—ã NumPy –∏ –¥—Ä—É–≥–∏–µ –Ω–µ—Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã
    –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ –¥–ª—è JSON
    """
    if isinstance(obj, (np.integer, np.int32, np.int64)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float32, np.float64)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return tuple(convert_to_serializable(item) for item in obj)
    elif hasattr(obj, '__dict__'):
        return convert_to_serializable(obj.__dict__)
    else:
        return obj


def safe_json_dumps(obj):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤ JSON"""
    return json.dumps(convert_to_serializable(obj), ensure_ascii=False)


def get_color_for_emotion(emotion_name, format='bgr'):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –¥–ª—è —ç–º–æ—Ü–∏–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
    if format == 'bgr':
        return EMOTION_COLORS_BGR.get(emotion_name, EMOTION_COLORS_BGR['default'])
    else:  # rgb
        return EMOTION_COLORS_RGB.get(emotion_name, EMOTION_COLORS_RGB['default'])


def detect_faces(image):
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    faces = face_cascade.detectMultiScale(
        gray,
        scaleFactor=1.1,
        minNeighbors=5,
        minSize=(30, 30),
        flags=cv2.CASCADE_SCALE_IMAGE
    )
    return faces


def preprocess_face_for_model(face_image):
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ª–∏—Ü–∞ –¥–ª—è –Ω–∞—Å—Ç–æ—è—â–µ–π –º–æ–¥–µ–ª–∏ (–Ω–µ –¥–µ–º–æ)"""
    try:
        if len(face_image.shape) == 3:
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = face_image

        # –£–ª—É—á—à–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        gray = cv2.equalizeHist(gray)
        gray = cv2.GaussianBlur(gray, (3, 3), 0)

        # –†–µ—Å–∞–π–∑ –¥–æ 48x48
        resized = cv2.resize(gray, (48, 48))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        normalized = resized.astype('float32') / 255.0

        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: (batch, height, width, channels)
        return np.expand_dims(normalized, axis=(0, -1))

    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–∏—Ü–∞: {e}")
        return np.ones((1, 48, 48, 1)) * 0.5


def predict_emotion_model(face_image):
    """–ù–∞—Å—Ç–æ—è—â–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Keras –º–æ–¥–µ–ª–∏"""
    if face_image.size == 0:
        return {
            "emotion": "Unknown",
            "confidence": 0.0,
            "all_predictions": []
        }

    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        processed = preprocess_face_for_model(face_image)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        predictions = emotion_model.predict(processed, verbose=0)

        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é —ç–º–æ—Ü–∏—é
        emotion_idx = np.argmax(predictions[0])
        confidence = float(predictions[0][emotion_idx])  # –Ø–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏
        emotion_name = emotion_labels.get(str(emotion_idx), f"Emotion_{emotion_idx}")

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        all_predictions = []
        for i, prob in enumerate(predictions[0]):
            emo_name = emotion_labels.get(str(i), f"Emotion_{i}")
            all_predictions.append({
                "emotion": emo_name,
                "probability": float(prob),  # –Ø–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float
                "color_bgr": get_color_for_emotion(emo_name, 'bgr'),
                "color_rgb": get_color_for_emotion(emo_name, 'rgb')
            })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º
        all_predictions.sort(key=lambda x: x["probability"], reverse=True)

        print(f"üé≠ –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞: {emotion_name} ({confidence:.2%})")

        return {
            "emotion": emotion_name,
            "confidence": confidence,
            "emotion_idx": int(emotion_idx),  # –Ø–≤–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int
            "color_bgr": get_color_for_emotion(emotion_name, 'bgr'),  # –î–ª—è OpenCV
            "color_rgb": get_color_for_emotion(emotion_name, 'rgb'),  # –î–ª—è HTML
            "all_predictions": all_predictions
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ–º–æ-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–∞–∫ –∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
        return predict_emotion_demo(face_image)


def predict_emotion_demo(face_image):
    """–î–µ–º–æ-–≤–µ—Ä—Å–∏—è"""
    if face_image.size == 0:
        return {
            "emotion": "Unknown",
            "confidence": 0.0,
            "all_predictions": []
        }

    try:
        if len(face_image.shape) == 3:
            gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
        else:
            gray = face_image

        height, width = gray.shape
        avg_brightness = float(gray.mean())
        contrast = float(gray.std())
        top_half = float(gray[:height // 2, :].mean())
        bottom_half = float(gray[height // 2:, :].mean())
        smile_ratio = float(bottom_half / (top_half + 1))
        eyebrow_region = gray[height // 4:height // 2, width // 4:3 * width // 4]
        eyebrow_darkness = float(255 - eyebrow_region.mean())

        emotion = "Neutral"
        confidence = 0.7

        if smile_ratio > 1.15:
            emotion = "Happy"
            confidence = min(0.9, 0.7 + (smile_ratio - 1.15) * 2)
        elif smile_ratio < 0.85:
            emotion = "Sad"
            confidence = min(0.85, 0.7 + (0.85 - smile_ratio) * 2)
        elif eyebrow_darkness > 100:
            emotion = "Angry"
            confidence = min(0.8, 0.6 + eyebrow_darkness / 255)
        elif contrast > 70:
            emotion = "Surprise"
            confidence = min(0.75, 0.6 + contrast / 255)
        elif avg_brightness < 80:
            emotion = "Fear"
            confidence = 0.65

        all_emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
        all_predictions = []

        for emo in all_emotions:
            if emo == emotion:
                prob = confidence
            else:
                prob = (1 - confidence) / (len(all_emotions) - 1) * 0.5

            all_predictions.append({
                "emotion": emo,
                "probability": float(prob),
                "color_bgr": get_color_for_emotion(emo, 'bgr'),
                "color_rgb": get_color_for_emotion(emo, 'rgb')
            })

        total = sum(p["probability"] for p in all_predictions)
        for p in all_predictions:
            p["probability"] /= total

        all_predictions.sort(key=lambda x: x["probability"], reverse=True)

        return {
            "emotion": emotion,
            "confidence": float(confidence),
            "emotion_idx": all_emotions.index(emotion),
            "color_bgr": get_color_for_emotion(emotion, 'bgr'),  # –î–ª—è OpenCV
            "color_rgb": get_color_for_emotion(emotion, 'rgb'),  # –î–ª—è HTML
            "all_predictions": all_predictions
        }

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–º–æ-–∞–Ω–∞–ª–∏–∑–∞: {e}")
        import random
        emotions = ['Happy', 'Sad', 'Neutral', 'Surprise', 'Angry']
        emotion = random.choice(emotions)

        return {
            "emotion": emotion,
            "confidence": float(random.uniform(0.6, 0.9)),
            "color_bgr": get_color_for_emotion(emotion, 'bgr'),
            "color_rgb": get_color_for_emotion(emotion, 'rgb'),
            "all_predictions": []
        }


def predict_emotion(face_image):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    if MODEL_LOADED and emotion_model is not None:
        # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Å—Ç–æ—è—â—É—é –º–æ–¥–µ–ª—å
        try:
            return predict_emotion_model(face_image)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –Ω–∞—Å—Ç–æ—è—â–µ–π –º–æ–¥–µ–ª–∏, –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –¥–µ–º–æ: {e}")
            return predict_emotion_demo(face_image)
    else:
        return predict_emotion_demo(face_image)


# ==================== API ENDPOINTS ====================

@app.get("/")
async def home(request: Request):
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return templates.TemplateResponse("emotion_detection.html", {
        "request": request,
        "model_loaded": MODEL_LOADED,
        "emotions": emotion_labels,
        "emotion_colors": EMOTION_COLORS_RGB,
        "default_threshold": 50
    })


@app.post("/detect")
async def detect_emotions(
        request: Request,
        file: UploadFile = File(...),
        confidence_threshold: float = Form(50.0),
        selected_emotions: str = Form(""),
        calculate_area: bool = Form(False)
):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π"""

    if not MODEL_LOADED:
        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "error": "–ú–æ–¥–µ–ª—å —ç–º–æ—Ü–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!"
        })

    try:
        threshold = confidence_threshold / 100.0

        if selected_emotions:
            selected_emotions_list = [emo.strip().lower() for emo in selected_emotions.split(",")]
            valid_emotions = [emo for emo in selected_emotions_list
                              if emo in [e.lower() for e in EMOTION_CLASSES]]
        else:
            valid_emotions = []

        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        contents = await file.read()
        nparr = np.frombuffer(contents, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        original_image = image.copy()

        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º –ª–∏—Ü–∞
        faces = detect_faces(image)
        detected_faces = []
        height, width = image.shape[:2]

        # –†–∞—Å—á–µ—Ç –ø–ª–æ—â–∞–¥–µ–π (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)
        face_areas = []

        for i, (x, y, w, h) in enumerate(faces):
            if w < 20 or h < 20:
                continue

            face_roi = image[y:y + h, x:x + w]
            emotion_result = predict_emotion(face_roi)

            if emotion_result["confidence"] >= threshold:
                if valid_emotions and emotion_result["emotion"].lower() not in valid_emotions:
                    continue

                face_data = {
                    "face_id": i + 1,
                    "emotion": emotion_result["emotion"],
                    "confidence": float(emotion_result["confidence"]),  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float
                    "box": [int(x), int(y), int(x + w), int(y + h)],  # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int
                    "color_bgr": emotion_result["color_bgr"],
                    "color_rgb": emotion_result["color_rgb"],
                    "all_predictions": emotion_result["all_predictions"]
                }

                # –†–∞—Å—á–µ—Ç –ø–ª–æ—â–∞–¥–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if calculate_area:
                    area_pixels = float(w * h)
                    relative_area_percent = float((area_pixels / (width * height)) * 100)
                    face_data.update({
                        "area_pixels": area_pixels,
                        "relative_area_percent": relative_area_percent,
                        "aspect_ratio": float(w / h) if h > 0 else 0.0
                    })
                    face_areas.append(area_pixels)

                detected_faces.append(face_data)

        # –†–∏—Å—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for face in detected_faces:
            x_min, y_min, x_max, y_max = face["box"]
            color_bgr = face["color_bgr"]

            cv2.rectangle(original_image, (x_min, y_min), (x_max, y_max), color_bgr, 3)

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–ª–æ—â–∞–¥–∏ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if calculate_area and "area_pixels" in face:
                area_text = f"Area: {face['area_pixels']:.0f} px¬≤"
                (area_width, area_height), _ = cv2.getTextSize(
                    area_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1
                )

                cv2.rectangle(original_image,
                              (x_min, y_max),
                              (x_min + area_width, y_max + area_height + 5),
                              color_bgr, -1)

                cv2.putText(original_image, area_text, (x_min, y_max + area_height),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)

            label = f"{face['face_id']}: {face['emotion']}: {face['confidence']:.2f}"
            (text_width, text_height), baseline = cv2.getTextSize(
                label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2
            )

            cv2.rectangle(original_image,
                          (x_min, y_min - text_height - 10),
                          (x_min + text_width, y_min),
                          color_bgr, -1)

            cv2.putText(original_image, label, (x_min, y_min - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        uploads_dir = "static/uploads"
        os.makedirs(uploads_dir, exist_ok=True)

        unique_id = str(uuid.uuid4())[:8]
        output_filename = f"detected_{unique_id}.jpg"
        original_filename = f"original_{unique_id}.jpg"

        cv2.imwrite(f"{uploads_dir}/{output_filename}", original_image)
        cv2.imwrite(f"{uploads_dir}/{original_filename}", image)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        emotion_stats = {}
        for face in detected_faces:
            emotion = face["emotion"]
            emotion_stats[emotion] = emotion_stats.get(emotion, 0) + 1

        image_info = {
            "width": int(width),
            "height": int(height),
            "format": file.content_type,
            "filename": file.filename
        }

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–ª–æ—â–∞–¥—è–º
        area_stats = {}
        if calculate_area and face_areas:
            area_stats = {
                "total_area_pixels": float(sum(face_areas)),
                "average_area_pixels": float(sum(face_areas) / len(face_areas) if face_areas else 0),
                "min_area_pixels": float(min(face_areas) if face_areas else 0),
                "max_area_pixels": float(max(face_areas) if face_areas else 0),
                "image_area_pixels": float(width * height),
                "faces_coverage_percent": float(
                    (sum(face_areas) / (width * height)) * 100 if (width * height) > 0 else 0)
            }

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–µ—Ä–∏–∞–ª–∏–∑—É–µ–º—ã–µ —Ç–∏–ø—ã
        safe_detected_faces = convert_to_serializable(detected_faces)
        safe_emotion_stats = convert_to_serializable(emotion_stats)
        safe_image_info = convert_to_serializable(image_info)
        safe_area_stats = convert_to_serializable(area_stats)
        safe_stats = convert_to_serializable({
            "total_faces_detected": len(faces),
            "faces_with_emotion": len(detected_faces),
            "min_confidence": f"{min([f['confidence'] for f in detected_faces]) * 100:.1f}%" if detected_faces else "0%",
            "image_size": f"{width}x{height}",
            "processing_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })

        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "detected_faces": safe_detected_faces,
            "emotion_stats": safe_emotion_stats,
            "image_url": f"/static/uploads/{output_filename}",
            "original_image_url": f"/static/uploads/{original_filename}",
            "total_detected": len(detected_faces),
            "total_faces": len(faces),
            "image_info": safe_image_info,
            "used_threshold": confidence_threshold,
            "used_emotions": ", ".join(valid_emotions) if valid_emotions else "–≤—Å–µ —ç–º–æ—Ü–∏–∏",
            "calculate_area": calculate_area,
            "area_stats": safe_area_stats,
            "processing_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "emotion_colors": EMOTION_COLORS_RGB,
            "stats": safe_stats,
            "results": safe_detected_faces,
            "emotion_distribution": safe_emotion_stats
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return templates.TemplateResponse("emotion_result.html", {
            "request": request,
            "error": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
        })


if __name__ == "__main__":
    import uvicorn

    print("=" * 60)
    print("üöÄ –°–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π –∑–∞–ø—É—â–µ–Ω–∞!")
    print(f"üìä –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {'‚úÖ' if MODEL_LOADED else '‚ùå'}")
    if MODEL_LOADED:
        print(f"üé≠ –≠–º–æ—Ü–∏–π: {len(emotion_labels)}")
        print(f"üìà –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: ~63%")
    print("üåê –î–æ—Å—Ç—É–ø–Ω–æ –ø–æ –∞–¥—Ä–µ—Å—É: http://localhost:8000")
    print("=" * 60)

    uvicorn.run(app, host="0.0.0.0", port=8000)
